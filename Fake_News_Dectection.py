# -*- coding: utf-8 -*-
"""Fake News Dectection .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ebGm84kwqUK4khXrxolgEFFLZ1Fw6XWl
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
df = pd.read_csv('/content/Fake.csv')

# Display the first few rows to understand the data
df.head()

# Combine 'title' and 'text' columns for feature extraction
df['content'] = df['title'] + " " + df['text']

# Drop unnecessary columns
df = df[['content', 'subject']]

# Binary classification: Consider "News" as real, other subjects as fake
df['label'] = df['subject'].apply(lambda x: 1 if x == 'News' else 0)

# Display the updated dataset
df.head(10)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['content'], df['label'], test_size=0.2, random_state=42)

# Vectorize the text data using TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Initialize models
log_reg = LogisticRegression()
decision_tree = DecisionTreeClassifier()
gradient_boosting = GradientBoostingClassifier()
random_forest = RandomForestClassifier()

# Train and evaluate models
models = {'Logistic Regression': log_reg,
          'Decision Tree': decision_tree,
          'Gradient Boosting': gradient_boosting,
          'Random Forest': random_forest}

for model_name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{model_name} Accuracy: {accuracy:.2f}")
    print(classification_report(y_test, y_pred))

# Initialize an empty dictionary to store accuracy scores
model_accuracies = {}
# Train and evaluate models, then store accuracies
for model_name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    accuracy = accuracy_score(y_test, y_pred)
    model_accuracies[model_name] = accuracy

# Extract model names and their accuracies
model_names = list(model_accuracies.keys())
accuracies = list(model_accuracies.values())

# Plot the accuracies
plt.figure(figsize=(10, 6))
plt.barh(model_names, accuracies, color='skyblue')
plt.xlabel('Accuracy')
plt.title('Model Accuracy Comparison')
plt.xlim(0, 1.5)  # Accuracy is between 0 and 1
plt.show()

# Create a small test dataset
data = {'title': ['Breaking News', 'Amazing discovery', 'Unbelievable story', 'Shocking revelation', 'Exciting event'],
        'text': ['This is the real news', 'Scientists have discovered a new element',
                 'The story is completely fabricated', 'It turns out the rumor is false',
                 'An exciting new development in tech'],
        'subject': ['News', 'News', 'Fake', 'Fake', 'News']}

# Create DataFrame
df_test = pd.DataFrame(data)

# Combine 'title' and 'text' columns for feature extraction
df_test['content'] = df_test['title'] + " " + df_test['text']

# Drop unnecessary columns
df_test = df_test[['content', 'subject']]

# Binary classification: Consider "News" as real, other subjects as fake
df_test['label'] = df_test['subject'].apply(lambda x: 1 if x == 'News' else 0)

df_test.head()